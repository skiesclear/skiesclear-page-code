---
title: "动态规划算法"
---
---
title: "动态规划算法"
---
---
title: "动态规划算法"
---
# 1. 简介
基于动态规划的强化学习算法主要有两种：
- 策略迭代（policy iteration）
	- 策略评估（policy evaluation）
		- 用贝尔曼期望方程来得到一个策略的状态价值函数，这是一个动态规划的过程
	- 策略提升（policy improvement）
- 价值迭代（value iteration）
	- 使用贝尔曼最优方程来进行动态规划，得到最终的最优状态价值。


基于动态规划的这两种强化学习算法要求事先知道环境的状态转移函数和奖励函数，也就是需要知道整个马尔可夫决策过程。而且策略迭代和价值迭代通常只适用于有限马尔可夫决策过程，即**状态空间和动作空间是离散且有限的**。

# 2. 策略迭代算法
> 策略迭代是策略评估和策略提升不断循环交替，直至最后得到最优策略的过程。
## 2.1. 策略评估
> 策略评估这一过程用来计算一个策略的状态价值函数。

考虑贝尔曼期望方程：
$$V^\pi(s)=\sum_{a\in A}\pi(a|s)\left(r(s,a)+\gamma\sum_{s'\in S}p(s'|s,a)V^\pi(s')\right)$$
当知道奖励函数和状态转移函数时，我们可以根据下一个状态的价值来计算当前状态的价值。因此，根据动态规划的思想，可以把计算下一个可能状态的价值当成一个子问题，把计算当前状态的价值看作当前问题。在得知子问题的解后，就可以求解当前问题。更一般的，考虑所有的状态，就变成了用上一轮的状态价值函数来计算当前这一轮的状态价值函数，即：
$$V^{k+1}(s)=\sum_{a\in A}\pi(a|s)\left(r(s,a)+\gamma\sum_{s'\in S}P(s'|s,a)V^k(s')\right)$$
可以选定任意初始值$V^0$。根据贝尔曼期望方程，可以得知是以上更新公式的一个不动点（fixed point）。事实上，可以证明当时，序列会收敛到，所以可以据此来计算得到一个策略的状态价值函数。考虑到计算代价，当某一轮$\max_{s\in\mathcal{S}}|V^{k+1}(s)-V^k(s)|$的值非常小就可以提前结束策略评估。

## 2.2. 策略提升
使用策略评估计算得到当前策略的状态价值函数之后，我们可以据此来改进该策略。
现在假设存在一个确定性策略$\pi^\prime$，在任意一个状态$s$下，都满足
$$Q^\pi(s,\pi^{\prime}(s))\geq V^\pi(s)$$
于是在任意状态$s$下，我们有
$$V^{\pi^{\prime}}(s)\geq V^\pi(s)$$这便是**策略提升定理（policy improvement theorem）**。于是我们可以直接贪心地在每一个状态选择动作价值最大的动作，也就是
$$\pi'(s)=\arg\max_aQ^\pi(s,a)=\arg\max_a\{r(s,a)+\gamma\sum_{s'}P(s'|s,a)V^\pi(s')\}$$

# 3. 价值迭代算法
策略迭代中的策略评估需要进行很多轮才能收敛得到某一策略的状态函数，这需要很大的计算量，尤其是在状态和动作空间比较大的情况下。试想一下，可能出现这样的情况：虽然状态价值函数还没有收敛，但是不论接下来怎么更新状态价值，策略提升得到的都是同一个策略。
价值迭代算法，它可以被认为是一种策略评估只进行了一轮更新的策略迭代算法。
价值迭代算法利用的是贝尔曼最优方程：
$$V^*(s)=\max_{a\in\mathcal{A}}\{r(s,a)+\gamma\sum_{s'\in\mathcal{S}}P(s'|s,a)V^*(s')\}$$
改写为迭代更新的方式：
$$V^{k+1}(s)=\max_{a\in\mathcal{A}}\{r(s,a)+\gamma\sum_{s'\in\mathcal{S}}P(s'|s,a)V^k(s')\}$$
价值迭代便是按照以上更新方式进行的。等到$V^{k+1}$和$V^{k}$相同时，它就是贝尔曼最优方程的不动点，此时对应着最优状态价值函数$V^\star$.
然后利用$\pi(s)=\arg\max_a\{r(s,a)+\gamma\sum_{s^{\prime}}p(s^{\prime}|s,a)V^{k+1}(s^{\prime})\}$从中恢复出最优策略即可。
