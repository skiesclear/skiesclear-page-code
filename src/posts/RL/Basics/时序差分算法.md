---
title: "时序差分算法"
---
---
title: "时序差分算法"
---
---
title: "时序差分算法"
---
# 1. 简介
大部分强化学习场景，马尔可夫决策过程的状态转移概率是无法写出来的，也就无法直接进行动态规划。这种情况下，智能体只能和环境进行交互，通过采样到的数据来学习，这类学习方法统称为**无模型的强化学习（model-free reinforcement learning）**。
无模型的强化学习算法不需要事先知道环境的奖励函数和状态转移函数，而是直接使用和环境交互的过程中采样到的数据来学习，这使得它可以被应用到一些简单的实际场景中。
- 在线策略学习
	- 在线策略学习要求使用在当前策略下采样得到的样本进行学习，一旦策略被更新，当前的样本就被放弃了
- 离线策略学习
	- 而离线策略学习使用经验回放池将之前采样得到的样本收集起来再次利用

# 2. 时序差分方法
时序差分是一种用来估计一个策略的价值函数的方法，它结合了蒙特卡洛和动态规划算法的思想。
- 相似于蒙特卡洛：从样本数据学习，事先不知道环境
- 相似于动态规划：利用后续状态的价值来更新当前状态的价值

不同于蒙特卡洛要等整个序列结束后才能计算这一次的回报，时序差分只需当前步结束后即可进行计算。具体来说，时序差分算法用当前获得的奖励加上下一个状态的价值估计来作为在当前状态会获得的回报，即
$$V(s_t)\leftarrow V(s_t)+\alpha[r_t+\gamma V(s_{t+1})-V(s_t)]$$
其中$r_t+\gamma V(s_{t+1})-V(s_t)$通常被称为**时序差分（temporal difference，TD）误差（error）**，时序差分算法将其与步长的乘积作为状态价值的更新量。

# 3. Sarsa算法
既然可以使用时序差分算法来估计价值函数，就可以用类似策略迭代的方法进行强化学习
策略迭代：
- 策略评估 时序差分算法来估计$V(s_t)$
- 策略提升 可以直接使用时序差分算法估计动作价值函数$Q$

$$Q(s_t,a_t)\leftarrow Q(s_t,a_t)+\alpha[r_t+\gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t)]$$

然后我们用贪婪算法来选取在某个状态下动作价值最大的那个动作，即$\arg\max_aQ(s,a)$。

这个简单的算法需要两个问题考虑：
1、需要使用极大量的样本进行更新。但我们可以忽略这一点，直接用一些样本来评估策略，然后就可以更新策略了。原因是策略提升可以在策略评估未完全进行的情况进行，这是**广义策略迭代**（generalized policy iteration）的思想。
2、如果在策略提升中一直根据贪婪算法得到一个确定性策略，可能会导致某些状态动作$(s,a)$对永远没有在序列中出现,以至于无法对其动作价值进行估计，进而无法保证策略提升后的策略比之前的好。
解决办法：采用$\varepsilon-$贪婪策略
$$\pi(a|s)=\begin{cases}\epsilon/|\mathcal{A}|+1-\epsilon&\quad\textit{如果}a=\arg\max_{a'}Q(s,a')\\\epsilon/|\mathcal{A}|&\quad\textit{其他动作}\end{cases}$$

# 4. 多步Sarsa算法
当前状态之后每一步的奖励而不使用任何价值估计，时序差分算法只利用一步奖励和下一个状态的价值估计。
蒙特卡洛算法：无偏（unbiased）的，但是具有较大的方差。
时序差分算法：有偏的，因为用到了下一个状态的价值而不是真实的价值。
结合二者的优势：**多步时序差分**
将
$$Q(s_t,a_t)\leftarrow Q(s_t,a_t)+\alpha[r_t+\gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t)]$$
替换为：
$$Q(s_t,a_t)\leftarrow Q(s_t,a_t)+\alpha[r_t+\gamma r_{t+1}+\cdots+\gamma^nQ(s_{t+n},a_{t+n})-Q(s_t,a_t)]$$

# 5. Q-learning算法
Q-learning 和 Sarsa 的最大区别在于 Q-learning 的时序差分更新方式为：
$$Q(s_t,a_t)\leftarrow Q(s_t,a_t)+\alpha[R_t+\gamma\max_aQ(s_{t+1},a)-Q(s_t,a_t)]$$

## 5.1. 在线策略算法与离线策略算法
我们称采样数据的策略为行为策略（behavior policy），称用这些数据来更新的策略为目标策略（target policy）。

在线策略（on-policy）算法表示行为策略和目标策略是同一个策略；
- Sarsa算法
离线策略（off-policy）算法表示行为策略和目标策略不是同一个策略。
- Q-learning