---
title: "马尔可夫决策过程"
---
---
title: "马尔可夫决策过程"
---
---
title: "马尔可夫决策过程"
---
# 1. 马尔可夫过程
## 1.1. 马尔可夫性质

且仅当某时刻的状态只取决于上一时刻的状态时，一个随机过程被称为具有马尔可夫性质（Markov property），用公式表示为$P(S_{t+1}|S_t)=P(S_{t+1}|S_1,\ldots,S_t)$

> 需要明确的是，具有马尔可夫性并不代表这个随机过程就和历史完全没有关系。因为虽然时刻$t+1$的状态只与时刻$t$的状态有关，但是时刻$t$的状态其实包含了时刻$t-1$的状态的信息，通过这种链式的关系，历史的信息被传递到了现在。

## 1.2. 马尔可夫过程
马尔可夫过程（Markov process）指具有马尔可夫性质的随机过程，也被称为马尔可夫链（Markov chain）

通常用元组$\langle\mathcal{S},\mathcal{P}\rangle$描述一个马尔可夫过程，其中$\mathcal{S}$是有限数量的状态集合，$\mathcal{P}$是状态转移矩阵（state transition matrix）。假设一共有个n状态，此时$\mathcal{S}=\{s_1,s_2,\ldots,s_n\}$。状态转移矩阵$\mathcal{P}$定义了所有状态对之间的转移概率，即
$$\mathcal{P}=\begin{bmatrix}P(s_1|s_1)&\cdots&P(s_n|s_1)\\\vdots&\ddots&\vdots\\P(s_1|s_n)&\cdots&P(s_n|s_n)\end{bmatrix}$$
矩阵中$\mathcal{P}$中第$i$行第$j$列元素$P(s_j|s_i)=P(S_{t+1}=s_j|S_t=s_i)$表示从状态$s_i$转移到状态$s_j$的概率，称$P(s^{\prime}|s)$为状态转移函数。

## 1.3. 马尔可夫奖励过程

在马尔可夫过程的基础上加入奖励函数$r$和折扣因子$\gamma$，就可以得到马尔可夫奖励过程(Markov reward process)。一个马尔可夫奖励过程由$\langle\mathcal{S},\mathcal{P},r,\gamma\rangle$构成。
其中
- r是奖励函数，某个状态$s$的奖励$r(s)$指转移到该状态时可以获得奖励的期望。
- $\gamma$是折扣因子（discount factor），$\gamma$的取值范围为$[0,1)$。引入折扣因子的理由为远期利益具有一定不确定性，有时我们更希望能够尽快获得一些奖励，所以我们需要对远期利益打一些折扣。接近 1 的$\gamma$更关注长期的累计奖励，接近 0 的$\gamma$更考虑短期奖励。

### 1.3.1. 回报
在一个马尔可夫奖励过程中，从第$t$时刻状态$S_t$开始，直到终止状态时，所有奖励的衰减之和称为回报$G_t$（Return）:
$$G_t=R_t+\gamma R_{t+1}+\gamma^2R_{t+2}+\cdots=\sum_{k=0}^\infty\gamma^kR_{t+k}$$

### 1.3.2. 价值函数
在马尔可夫奖励过程中，一个状态的期望回报（即从这个状态出发的未来累积奖励的期望）被称为这个状态的**价值**（value）。所有状态的价值就组成了**价值函数**（value function），价值函数的输入为某个状态，输出为这个状态的价值。我们将价值函数写成$V(s)=\mathbb{E}[G_t|S_t=s]$，展开为：

$$\begin{aligned}
V(s)& =\mathbb{E}[G_t|S_t=s] \\
&=\mathbb{E}[R_t+\gamma R_{t+1}+\gamma^2R_{t+2}+\ldots|S_t=s] \\
&=\mathbb{E}[R_t+\gamma(R_{t+1}+\gamma R_{t+2}+\ldots)|S_t=s] \\
&=\mathbb{E}[R_t+\gamma G_{t+1}|S_t=s] \\
&=\mathbb{E}[R_t+\gamma V(S_{t+1})|S_t=s]
\end{aligned}$$
在上式的最后一个等号中，一方面，即时奖励的期望正是奖励函数的输出，即$\mathbb{E}[R_t|S_t=s]=r(s)$；另一方面，等式中剩余部分$\mathbb{E}[\gamma V(S_{t+1}|S_t=s)]$可以根据从状态出发的转移概率得到，即可以得到
$$V(s)=r(s)+\gamma\sum_{s'\in S}p(s'|s)V(s')$$
上式就是马尔可夫奖励过程中非常有名的**贝尔曼方程**（Bellman equation），对每一个状态都成立。
若一个马尔可夫奖励过程一共有个状态，即$\mathcal{S}=\{s_1,s_2,\ldots,s_n\}$，我们将所有状态的价值表示成一个列向量$\mathcal{V}=[V(s_1),V(s_2),\ldots,V(s_n)]^T$，同理，将奖励函数写成一个列向量$\mathcal{R}=[r(s_1),r(s_2),\ldots,r(s_n)]^T$。于是我们可以将贝尔曼方程写成矩阵的形式：
$$\mathcal{V}=\mathcal{R}+\gamma\mathcal{PV}$$
$$\begin{aligned}\begin{bmatrix}V(s_1)\\V(s_2)\\\cdots\\V(s_n)\end{bmatrix}=\begin{bmatrix}r(s_1)\\r(s_2)\\\cdots\\r(s_n)\end{bmatrix}+\gamma\begin{bmatrix}P(s_1|s_1)&p(s_2|s_1)&\ldots&P(s_n|s_1)\\P(s_1|s_2)&P(s_2|s_2)&\ldots&P(s_n|s_2)\\\cdots\\P(s_1|s_n)&P(s_2|s_n)&\ldots&P(s_n|s_n)\end{bmatrix}\begin{bmatrix}V(s_1)\\V(s_2)\\\ldots\\V(s_n)\end{bmatrix}\end{aligned}$$
得到解析解：
$$\begin{aligned}\mathcal{V}&=\mathcal{R}+\gamma\mathcal{P}\mathcal{V}\\(I-\gamma\mathcal{P})\mathcal{V}&=\mathcal{R}\\\mathcal{V}&=(I-\gamma\mathcal{P})^{-1}\mathcal{R}\end{aligned}$$
复杂度为$O(n^3)$。

# 2. 马尔可夫决策过程
如果有一个外界的“刺激”来共同改变这个随机过程，就有了马尔可夫决策过程（Markov decision process，MDP）。我们将这个来自外界的刺激称为智能体（agent）的动作，在马尔可夫奖励过程（MRP）的基础上加入动作，就得到了马尔可夫决策过程（MDP）。马尔可夫决策过程由元组$\langle\mathcal{S},\mathcal{A},P,r,\gamma\rangle$构成，其中：
- $\mathcal{A}$是动作的集合
- $r(s,a)$是奖励函数，此时奖励可以同时取决于状态$s$和动作$a$，在奖励函数只取决于状态$s$时，则退化为$r(s)$
- $P(s^{\prime}|s,a)$是状态转移函数，表示在状态$s$执行动作$a$之后到达状态$s^{\prime}$的概率

## 2.1. 策略
智能体的策略（Policy）通常用字母$\pi$表示。策略$\pi(a|s)=P(A_t=a|S_t=s)$是一个函数，表示在输入状态情况下采取动作$a$的概率。
- 一个策略是确定性策略（deterministic policy）时，它在每个状态时只输出一个确定性的动作，即只有该动作的概率为 1，其他动作的概率为 0
- 一个策略是随机性策略（stochastic policy）时，它在每个状态时输出的是关于动作的概率分布，然后根据该分布进行采样就可以得到一个动作。

## 2.2. 状态价值函数
从状态$s$出发遵循策略$\pi$能获得的期望回报，数学表达为：
$$V^\pi(s)=\mathbb{E}_\pi[G_t|S_t=s]$$
## 2.3. 动作价值函数
用$Q^\pi(s,a)$表示在 MDP 遵循策略$\pi$时，对当前状态$s$执行动作$a$得到的期望回报：
$$Q^\pi(s,a)=\mathbb{E}_\pi[G_t|S_t=s,A_t=a]$$

**状态价值函数和动作价值函数之间的关系**：在使用策略$\pi$中，状态$s$的价值等于在该状态下基于策略$\pi$采取所有动作的概率与相应的价值相乘再求和的结果：
$$V^\pi(s)=\sum_{a\in A}\pi(a|s)Q^\pi(s,a)$$

使用策略$\pi$时，状态$s$下采取动作$a$的价值等于即时奖励加上经过衰减后的所有可能的下一个状态的状态转移概率与相应的价值的乘积：
$$Q^\pi(s,a)=r(s,a)+\gamma\sum_{s^{\prime}\in S}P(s^{\prime}|s,a)V^\pi(s^{\prime})$$
## 2.4. 贝尔曼期望方程
两个价值函数的贝尔曼期望方程（Bellman Expectation Equation）：
$$\begin{aligned}
V^{\pi}(s)& =\mathbb{E}_\pi[R_t+\gamma V^\pi(S_{t+1})|S_t=s] \\
&=\sum_{a\in A}\pi(a|s)\left(r(s,a)+\gamma\sum_{s^{\prime}\in S}p(s^{\prime}|s,a)V^{\pi}(s^{\prime})\right) \\
Q^{\pi}(s,a)& =\mathbb{E}_\pi[R_t+\gamma Q^\pi(S_{t+1},A_{t+1})|S_t=s,A_t=a] \\
&=r(s,a)+\gamma\sum_{s'\in S}p(s'|s,a)\sum_{a'\in A}\pi(a'|s')Q^{\pi}(s',a')
\end{aligned}$$

# 3. 蒙特卡洛方法
蒙特卡洛方法（Monte-Carlo methods）也被称为统计模拟方法，是一种基于概率统计的数值计算方法。

# 4. 占用度量
定义 MDP 的初始状态分布为$\nu_{0}(s)$，在有些资料中，初始状态分布会被定义进 MDP 的组成元素中。我们用$P_{t}^{\pi}(s)$表示采取策略$\pi$使得智能体在时刻$t$状态为$s$的概率，所以我们有$P_0^\pi(s)=\nu_0(s)$，然后就可以定义一个策略的**状态访问分布（state visitation distribution）**：
$$\nu^\pi(s)=(1-\gamma)\sum_{t=0}^\infty\gamma^tP_t^\pi(s)$$
$1-\gamma$是用来使得概率加和为 1 的归一化因子。
理论上在计算该分布时需要交互到无穷步之后，但实际上智能体和 MDP 的交互在一个序列中是有限的。不过我们仍然可以用以上公式来表达状态访问概率的思想，状态访问概率有如下性质：
$$\nu^\pi(s')=(1-\gamma)\nu_0(s')+\gamma\int P(s'|s,a)\pi(a|s)\nu^\pi(s)dsda$$
- $(1-\gamma)\nu_0(s^\prime)$：表示初始状态下直接处于$s^\prime$的概率。这部分概率被$1-\gamma$缩放，其中$\gamma$是折扣因子，表示对未来的重视程度，当$\gamma$接近1时，更重视未来。
- $\gamma\int P(s'|s,a)\pi(a|s)\nu^\pi(s)dsda$：从所有其他状态$s$通过动作a转移到$s^\prime$的概率。这里使用积分来表里所有可能的状态和动作，这是因为我们是在无穷步的假设下进行的。

策略的**占用度量（occupancy measure）**：
$$\rho^\pi(s,a)=(1-\gamma)\sum_{t=0}^\infty\gamma^tP_t^\pi(s)\pi(a|s)$$
它表示动作状态对$(s,a)$被访问到的概率。

> 两个定理

- 定理1：智能体分别以策略$\pi_1$和$\pi_2$和同一个 MDP 交互得到的占用度量$\rho^{\pi_1}$和$\rho^{\pi_2}$满足:
$$\rho^{\pi_1}=\rho^{\pi_2}\iff\pi_1=\pi_2$$
- 定理 2：给定一合法占用度量$\rho$，可生成该占用度量的唯一策略是
$$\pi_\rho=\frac{\rho(s,a)}{\sum_{a^\prime}\rho(s,a^\prime)}$$
# 5. 最优策略
定义策略之间的偏序关系：当且仅当对于任意的状态$s$都有$V^{\pi}(s)\geq V^{\pi^{\prime}}(s)$ ，记$\pi>\pi^\prime$。于是在有限状态和动作集合的 MDP 中，至少存在一个策略比其他所有策略都好或者至少存在一个策略不差于其他所有策略，这个策略就是**最优策略（optimal policy）**。最优策略可能有很多个，我们都将其表示为$\pi^{\star}(s)$。

最优策略都有相同的状态价值函数，我们称之为**最优状态价值函数**:
$$V^*(s)=\max_\pi V^\pi(s),\quad\forall s\in\mathcal{S}$$**最优动作价值函数**:
$$Q^*(s,a)=\max_\pi Q^\pi(s,a),\quad\forall s\in\mathcal{S},a\in\mathcal{A}$$为了使$Q^{\pi}(s,a)$最大，我们需要在当前的状态动作对$(s,a)$之后都执行最优策略。
最优状态价值函数和最优动作价值函数之间的关系：
$$Q^*(s,a)=r(s,a)+\gamma\sum_{s^{\prime}\in S}P(s^{\prime}|s,a)V^*(s^{\prime})$$
另一方面，最优状态价值是选择此时使最优动作价值最大的那一个动作时的状态价值：
$$V^*(s)=\max_{a\in\mathcal{A}}Q^*(s,a)$$

## 5.1. 贝尔曼最优方程
$$V^*(s)=\max_{a\in\mathcal{A}}\{r(s,a)+\gamma\sum_{s'\in\mathcal{S}}p(s'|s,a)V^*(s')\}$$
$$Q^*(s,a)=r(s,a)+\gamma\sum_{s'\in\mathcal{S}}p(s'|s,a)\max_{a'\in\mathcal{A}}Q^*(s',a')$$
