---
title: "策略梯度算法"
---
---
title: "策略梯度算法"
---
---
title: "策略梯度算法"
---
# 1. 简介
## 1.1. 强化学习相关符号
- $\pi$ :策略（Policy）函数，参数常为环境返回的状态，输出为一个具体的动作，可知$a=\pi(s) \in A$
- $r$：采取动作$a$后获得的及时奖励
- $G$：累积奖赏，$G_t$表示从时刻t开始到游戏结束的累积奖赏。
$$G_t=\sum_{T=t}^{\infty}r_t$$
- $Q_\pi(s,a)$：状态-动作值函数（state-action value function），有时也简称动作值函数。即在t时刻，状态$s$下，采取动作$a$，使用策略$\pi$预计获得的累积奖赏的期望值。
$$Q_\pi(s,a)=\mathbb{E}[G_t|S_t=s,A_t=a]$$
- $V_\pi(s)$：状态值函数（state value function），即在t时刻，状态s下使用策略$\pi$预计获得奖赏的**期望值**。
$$V_\pi(s)=\mathbb{E}[G_t|S_t=s]$$
## 1.2. 生成策略的方法：基于值
> Value-based

Value-based方法就是根据$V_\pi(s)$和$Q_\pi(s,a)$两个函数，求出最大化奖赏的策略$\pi$。即
$$\pi_*=\arg\max_\pi \mathrm{V}_\pi(\mathrm{s})$$
或者
$$\pi_*=\arg\max_\pi Q_\pi(s,a)$$
也就是说，遍历所有的状态和动作，找到最大化值函数QV的策略。
具体方法：
动态规划，蒙特卡洛算法，时间差分算法（Q-learning，DQN，Sarsa）

## 1.3. 生成策略的方法：基于策略
> Policy-based


在Value-based方法中，策略是$\pi(s)$，只与状态有关，过程为：通过值函数的反馈修改策略，在根据策略计算值函数。
在Policy-based方法中，策略是$\pi(s|a,\theta)$，添加了一组参数$\theta$。目的就不再是基于VQ函数间接优化$\pi(s)$，二十通过优化参数$\theta$直接对策略$\pi_\theta$进行优化。

# 2. 策略梯度
策略学习的目标函数：
$$J(\theta)=\mathbb{E}_{s_0}[V^{\pi_0}(s_0)]$$
有了目标函数，我们将目标函数对策略$\theta$求导，得到导数后，就可以用梯度上升方法来最大化这个目标函数，从而得到最优策略。
$$
\begin{aligned}
\nabla_\theta J(\theta) & \propto \sum_{s \in S} \nu^{\pi_\theta}(s) \sum_{a \in A} Q^{\pi_\theta}(s, a) \nabla_\theta \pi_\theta(a \mid s) \\
& =\sum_{s \in S} \nu^{\pi_\theta}(s) \sum_{a \in A} \pi_\theta(a \mid s) Q^{\pi_\theta}(s, a) \frac{\nabla_\theta \pi_\theta(a \mid s)}{\pi_\theta(a \mid s)} \\
& =\mathbb{E}_{\pi_\theta}\left[Q^{\pi_\theta}(s, a) \nabla_\theta \log \pi_\theta(a \mid s)\right]
\end{aligned}
$$
$$\nu v \propto$$