---
title: 强化学习
---
---
title: 强化学习
---
# 1. 强化学习基础
>在机器学习领域，有一类重要的任务和人生选择很相似，即序贯决策（sequential decision making）任务。决策和预测任务不同，决策往往会带来“后果”，因此决策者需要为未来负责，在未来的时间点做出进一步的决策。实现序贯决策的机器学习方法就是本书讨论的主题—强化学习（reinforcement learning）。预测仅仅产生一个针对输入数据的信号，并期望它和未来可观测到的信号一致，这不会使未来情况发生任何改变。

## 1.1. 什么是强化学习
> 强化学习是机器通过与环境交互来实现目标的一种计算方法。
> 这种交互是迭代进行的，机器的目标是最大化在多轮交互过程中获得的累积奖励的期望。

强化学习用智能体（agent）这个概念来表示做决策的机器。相比于有监督学习中的“模型”，强化学习中的“智能体”强调机器不但可以感知周围的环境信息，还可以通过做决策来直接改变这个环境，而不只是给出一些预测信号。

智能体与环境之间交互：
在每一轮交互中，智能体感知到环境目前所处的状态，经过自身的计算给出本轮的动作，将其作用到环境中；环境得到智能体的动作后，产生相应的即时奖励信号并发生相应的状态转移。智能体则在下一轮交互中感知到新的环境状态，依次类推。
![[Pasted image 20240808220055.png|500]]

## 1.2. 强化学习的环境
强化学习的智能体是在和一个动态环境的交互中完成序贯决策的。
我们说一个环境是动态的，意思就是它会随着某些因素的变化而不断演变，这在数学和物理中往往用随机过程来刻画。对于一个随机过程，其最关键的要素就是状态以及状态转移的条件概率分布。这就好比一个微粒在水中的布朗运动可以由它的起始位置以及下一刻的位置相对当前位置的条件概率分布来刻画。

如果在环境这样一个自身演变的随机过程中加入一个外来的干扰因素，即智能体的动作，那么环境的下一刻状态的概率分布将由当前状态和智能体的动作来共同决定：
$$下一状态\sim{P(\cdot|当前状态，智能体的动作)}$$
由此看出，与面向决策任务的智能体进行交互的环境是一个动态的随机过程，其未来状态的分布由当前状态和智能体决策的动作来共同决定，并且每一轮状态转移都伴随着两方面的随机性：一是智能体决策的动作的随机性，二是环境基于当前状态和智能体动作来采样下一刻状态的随机性。


## 1.3. 强化学习的目标
根据环境的动态性我们可以知道，即使环境和智能体策略不变，智能体的初始状态也不变，智能体和环境交互产生的结果也很可能是不同的，对应获得的回报也会不同。
价值的计算有些复杂，因为需要对交互过程中每一轮智能体采取动作的概率分布和环境相应的状态转移的概率分布做积分运算。

## 1.4. 强化学习中的数据
在强化学习中，数据是在智能体与环境交互的过程中得到的。如果智能体不采取某个决策动作，那么该动作对应的数据就永远无法被观测到，所以当前智能体的训练数据来自之前智能体的决策结果。

强化学习中有一个关于数据分布的概念，叫作占用度量（occupancy measure）。
归一化的占用度量用于衡量在一个智能体决策与一个动态环境的交互过程中，采样到一个具体的状态动作对（state-action pair）的概率分布。
占用度量有一个很重要的性质：给定两个策略及其与一个动态环境交互得到的两个占用度量，那么当且仅当这两个占用度量相同时，这两个策略相同。

根据占用度量这一重要的性质，我们可以领悟到强化学习本质的思维方式。
- 强化学习的策略在训练中会不断更新，其对应的数据分布（即占用度量）也会相应地改变。因此，强化学习的一大难点就在于，智能体看到的数据分布是随着智能体的学习而不断发生改变的。
- 由于奖励建立在状态动作对之上，一个策略对应的价值其实就是一个占用度量下对应的奖励的期望，因此寻找最优策略对应着寻找最优占用度量。


## 1.5. 强化学习的独特性
有监督学习和强化学习的区别：
对于一般的有监督学习任务，我们的目标是找到一个最优的模型函数，使其在训练数据集上最小化一个给定的损失函数。在训练数据独立同分布的假设下，这个优化目标表示最小化模型在整个数据分布上的泛化误差（generalization error），用简要的公式可以概括为：
$$最优策略=arg\min_{模型}\mathbb{E}_{(特征，标签)\sim数据分布}[损失函数(标签，模型(特征))]$$
*arg表示在所有可能的模型参数中，找到使得期望损失函数最小化的那组参数。*

相比之下，强化学习任务的最终优化目标是最大化智能体策略在和动态环境交互过程中的价值。
$$最优策略=arg\max_{策略}\mathbb{E}_{(状态，动作)\sim策略的占用度量}[奖励函数(状态，动作)]$$
两者的相似点和不同点：
- 有监督学习和强化学习的优化目标相似，即都是在优化某个数据分布下的一个分数值的期望。
- 二者优化的途径是不同的，有监督学习直接通过优化模型对于数据特征的输出来优化目标，即修改目标函数而数据分布不变；强化学习则通过改变策略来调整智能体和环境交互数据的分布，进而优化目标，即修改数据分布而目标函数不变。

综上所述，一般有监督学习和强化学习的范式之间的区别为：

- 一般的有监督学习关注寻找一个模型，使其在给定数据分布下得到的损失函数的期望最小；
- 强化学习关注寻找一个智能体策略，使其在与动态环境交互的过程中产生最优的数据分布，即最大化该分布下一个给定奖励函数的期望。

# 2. 多臂老虎机
强化学习关注智能体和环境交互过程中的学习，这是一种试错型学习（trial-and-error learning）范式。
> 试错型学习（trial-and-error learning）是一种学习方法，通过尝试不同的行动或策略，并根据结果来调整未来的行为。它通常在没有明确指导或事先知识的情况下使用，特别是当面对新问题或不确定环境时。这种学习方法涉及通过反复尝试和观察结果来逐步改进，而不是依靠事先掌握的知识或理论。

## $\epsilon -贪心算法$
 $\epsilon$-贪婪算法在完全贪婪算法的基础上添加了噪声，每次以概率$1-\epsilon$选择以往经验中期望奖励估值最大的那根拉杆（利用），以概率 $\epsilon$随机选择一根拉杆（探索）:
$$a_t = \begin{cases}
argmax_{a \in \mathcal{A}} \hat{Q}(a), & \text{采样概率： } 1 - \epsilon, \\ \text{从}\mathcal{A}\text{中随便选择}, & \text{采样概率： } \epsilon.
\end{cases}$$

f(x)=\left\{
\begin{aligned}
x & =  \cos(t) \\
y & =  \sin(t) \\
z & =  \frac xy
\end{aligned}
\right.
$$


