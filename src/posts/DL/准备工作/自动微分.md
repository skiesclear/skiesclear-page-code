自动微分使系统能够随后反向传播梯度。 这里，反向传播（backpropagate）意味着跟踪整个计算图，填充关于每个参数的偏导数。

## 0.1. 简单例子

> requires_grad=True 的作用是让 backward 可以追踪这个参数并且计算它的梯度


```
# 
import torch

x = torch.arange(4.0, requires_grad=True)
print(x.grad) # None
y = 2 * torch.dot(x, x)
y.backward()
print(x.grad)
x.grad.zero_()
print(x.grad)
z = x.sum()
z.backward()
print(x.grad)
```

## 0.2. 非标量变量的反向传播
```
# 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。
# 本例只想求偏导数的和，所以传递一个1的梯度是合适的
x.grad.zero_()
y = x * x
# 等价于y.backward(torch.ones(len(x)))
y.sum().backward()
x.grad
```

如果是标量对向量求导(scalar对tensor求导)，那么就可以保证上面的计算图的根节点只有一个，此时不用引入gradient参数，直接调用backward函数即可
如果是(向量)矩阵对(向量)矩阵求导(tensor对tensor求导)，实际上是先求出Jacobian矩阵中每一个元素的梯度值(每一个元素的梯度值的求解过程对应上面的计算图的求解方法)，然后将这个Jacobian矩阵与gradient参数对应的矩阵进行对应的点乘，得到最终的结果。

## 0.3. 分离计算
有时，我们希望将某些计算移动到记录的计算图之外。 例如，假设y是作为x的函数计算的，而z则是作为y和x的函数计算的。 想象一下，我们想计算z关于x的梯度，但由于某种原因，希望将y视为一个常数， 并且只考虑到x在y被计算后发挥的作用。
解决办法：
这里可以分离y来返回一个新变量u，该变量与y具有相同的值， 但丢弃计算图中如何计算y的任何信息。 换句话说，梯度不会向后流经u到x。 因此，下面的反向传播函数计算$z=u*x$关于x的偏导数，同时将u作为常数处理， 而不是$z=x*x*x$关于x的偏导数。
```
x.grad.zero_()
y = x * x
u = y.detach()
z = u * x

z.sum().backward()
x.grad == u
```
> tensor.detac()返回一个新的tensor，从当前计算图中分离下来。但是仍指向原变量的存放位置，不同之处只是requirse_grad为false.得到的这个tensir永远不需要计算器梯度，不具有grad.
   即使之后重新将它的requires_grad置为true,它也不会具有梯度grad.这样我们就会继续使用这个新的tensor进行计算，后面当我们进行反向传播时，到该调用detach()的tensor就会停止，不能再继续向前进行传播.

注意：
使用detach返回的tensor和原始的tensor共同一个内存，即一个修改另一个也会跟着改变。