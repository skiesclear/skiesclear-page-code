## 0.1. 标量
```
x = torch.tensor(3.0)
y = torch.tensor(2.0)
```
## 0.2. 向量
```
x = torch.arange(4)
```
## 0.3. 长度、维度和形状
向量的长度通常称为向量的维度（dimension）。
```
len(x)
x.shape
```
## 0.4. 矩阵
```
A = torch.arange(20).reshape(5, 4)

A.T
```
## 0.5. 张量
就像向量是标量的推广，矩阵是向量的推广一样
```
X = torch.arange(24).reshape(2, 3, 4)
```
任何按元素的一元运算都不会改变其操作数的形状。 同样，给定具有相同形状的任意两个张量，任何按元素二元运算的结果都将是相同形状的张量。
例如：
```
 A + B, A*B,a + X, (a * X).shape
```
## 0.6. 降维
默认情况下，调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量。
```
x.sum()
```
还可以指定张量沿哪一个轴来通过求和降低维度。为了通过求和所有行的元素来降维（轴0），可以在调用函数时指定axis=0。 
```
A_sum_axis0 = A.sum(axis=0)
A.mean(axis=0)
```
## 0.7. 非降维求和
```
sum_A = A.sum(axis=1, keepdims=True)
```
由于sum_A在对每行进行求和后仍保持两个轴，我们可以通过广播将A除以sum_A。(这样相当于得到，一行中所有元素的占比)
```
A / sum_A
```
想沿某个轴计算A元素的累积总和
```
A.cumsum(axis=0)
```

## 0.8. 点积
给定两个向量$\boldsymbol{x,y}\in \mathbb{R}^d$,它们的点积$\boldsymbol{x}^T\boldsymbol{y}$是 相同位置的按元素乘积的和
```
torch.dot()
```
## 0.9. 矩阵-向量积
```
torch.mv()  # 矩阵*向量  矩阵中每一行元素与向量对应相乘，最后得到一个列向量
```
## 0.10. 矩阵-矩阵乘法
```
torch.mm()
```

## 0.11. 范数
线性代数中最有用的一些运算符是范数（norm）。 非正式地说，向量的范数是表示一个向量有多大。 这里考虑的大小（size）概念不涉及维度，而是分量的大小。
在线性代数中，向量范数是将向量映射到标量的函数f
向量范数的性质：
性质一：如果我们按常数因子$\alpha$缩放向量的所有元素， 其范数也会按相同常数因子的绝对值缩放：
$$f(\alpha\boldsymbol{x})=|\alpha|f(\boldsymbol{x})$$
性质二：三角不等式
$$f(\boldsymbol{x}+\boldsymbol{y})\le f(\boldsymbol{x})+f(\boldsymbol{y})$$
性质三：范数必须是非负的

$L_1$范数：绝对值函数和按元素求和
```
torch.abs(u).sum()
```
$L_2$范数($||\boldsymbol{x}||_2\text{等同于}||\boldsymbol{x}||$)：向量元素平方和的平方根:
```
torch.norm(u)
```