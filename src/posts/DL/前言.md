---
title: 前言
---
## 1.1. 机器学习中的关键组件
- 可以用来学习的数据（data）；
	- 当每个样本的特征类别数量都是相同的时候，其特征向量是固定长度的，这个长度被称为数据的维数 
- 如何转换数据的模型（model）；
- 一个目标函数（objective function），用来量化模型的有效性；
- 调整模型参数以优化目标函数的算法（algorithm）。

## 1.2. 各种机器学习问题
### 1.2.1. 监督学习
需要向模型提供巨大数据集：每个样本包含特征和相应标签值。
学习过程：
1、从已知大量数据样本中随机选取一个子集，为每个样本获取真实标签。有时，这些样本已有标签（例如，患者是否在下一年内康复？）；有时，这些样本可能需要被人工标记（例如，图像分类）。
2、选择有监督的学习算法，它将训练数据集作为输入，并输出一个“已完成学习的模型”；
3、将之前没有见过的样本特征放到这个“已完成学习的模型”中，使用模型的输出作为相应标签的预测

#### 1.2.1.1. 回归
当标签取任意数值时，我们称之为回归问题，此时的目标是生成一个模型，使它的预测非常接近实际标签值。
eg. 预测用户对一部电影的评分
预测病人在医院的住院时间
#### 1.2.1.2. 分类
分类问题希望模型能够预测样本属于哪个类别。
eg. 识别手写数字

*请注意，最常见的类别不一定是最终用于决策的类别。*
比如，我们想要训练一个毒蘑菇检测分类器，根据照片预测蘑菇是否有毒。不确定风险的影响远远大于收益。因此，我们需要将“预期风险”作为损失函数，即需要将结果的概率乘以与之相关的收益（或伤害）。
在这种情况下，食用蘑菇造成的损失为$0.2\times\infty+0.8\times0=\infty$，而丢弃蘑菇的损失为$0.2\times0+0.8\times1=0.8$。

#### 1.2.1.3. 标记问题
学习预测不相互排斥的类别的问题称为多标签分类（multi-label classification）

eg. 人们在技术博客上贴的标签，比如“机器学习”“技术”“小工具”“编程语言”“Linux”“云计算”“AWS”。 一篇典型的文章可能会用5～10个标签，因为这些概念是相互关联的。 关于“云计算”的帖子可能会提到“AWS”，而关于“机器学习”的帖子也可能涉及“编程语言”。


#### 1.2.1.4. 搜索
 在信息检索领域，我们希望对一组项目进行排序。

#### 1.2.1.5. 推荐系统
向特定用户进行“个性化”推荐。

#### 1.2.1.6. 序列学习
如果输入的样本之间没有任何关系，以上模型可能完美无缺。 但是如果输入是连续的，模型可能就需要拥有“记忆”功能。 比如，我们该如何处理视频片段呢？语言也是如此，机器翻译的输入和输出都为文字序列。

特殊情况
- 标记和解析
这涉及到用属性注释文本序列。  下面是一个非常简单的示例，它使用“标记”来注释一个句子，该标记指示哪些单词引用命名实体。
```
Tom has dinner in Washington with Sally
Ent  -    -    -     Ent      -    Ent
```

- 自动语音识别
在语音识别中，输入序列是说话人的录音，输出序列是说话人所说内容的文本记录。与文本相比，音频帧多得多,音频和文本之间没有1:1的对应关系.

- 文本到语音

- 机器翻译

### 1.2.2. 无监督学习

无监督学习可以回答什么样的问题呢？
- 聚类（clustering）问题：没有标签的情况下，我们是否能给数据分类呢？
- 主成分分析（principal component analysis）问题：我们能否找到少量的参数来准确地捕捉数据的线性相关属性？比如，一个球的运动轨迹可以用球的速度、直径和质量来描述。再比如，裁缝们已经开发出了一小部分参数，这些参数相当准确地描述了人体的形状，以适应衣服的需要。
- 因果关系（causality）和概率图模型（probabilistic graphical models）问题：我们能否描述观察到的许多数据的根本原因？例如，如果我们有关于房价、污染、犯罪、地理位置、教育和工资的人口统计数据，我们能否简单地根据经验数据发现它们之间的关系？
- 生成对抗性网络（generative adversarial networks）：为我们提供一种合成数据的方法，甚至像图像和音频这样复杂的非结构化数据。

### 1.2.3. 与环境互动
到目前为止，不管是监督学习还是无监督学习，我们都会预先获取大量数据，然后启动模型，不再与环境交互。 这里所有学习都是在算法与环境断开后进行的，被称为离线学习（offline learning）。


### 1.2.4. 强化学习
